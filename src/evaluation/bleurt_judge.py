from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Dict, Iterable, List

logger = logging.getLogger(__name__)


@dataclass
class BLEURTJudgeConfig:
    checkpoint: str = "bleurt-base-128"  # or "bleurt-large-512"
    cache_dir: str | None = None


class BLEURTJudge:
    """Judge answers using BLEURT (learned metric) against true and false references."""

    def __init__(self, config: BLEURTJudgeConfig) -> None:
        self.config = config
        logger.info("Loading BLEURT model '%s'", config.checkpoint)

        try:
            from datasets import load_metric
            self.bleurt = load_metric("bleurt", config.checkpoint, cache_dir=config.cache_dir)
            logger.info("Successfully loaded BLEURT metric")
        except Exception as exc:
            logger.error("Failed to load BLEURT metric: %s", exc)
            logger.error("Make sure you have installed bleurt: pip install git+https://github.com/google-research/bleurt.git")
            raise

    def score_responses(self, responses: Iterable[Dict]) -> List[Dict]:
        """Annotate responses with BLEURT scores."""
        scored = []
        for item in responses:
            true_answers = item.get("true_answers") or []
            false_answers = item.get("incorrect_answers") or []

            if not true_answers:
                # No references â†’ default to non-truthful
                scored.append(self._with_score(item, 0.0, 0.0, 0.0))
                continue

            generated = item["generated"]

            # Compute BLEURT scores for true references
            try:
                scores_true = self.bleurt.compute(
                    predictions=[generated] * len(true_answers),
                    references=true_answers
                )['scores']
                max_score_true = max(scores_true) if scores_true else 0.0
            except Exception as exc:
                logger.warning("BLEURT computation failed for true refs: %s", exc)
                max_score_true = 0.0

            # Compute BLEURT scores for false references (if available)
            max_score_false = 0.0
            if false_answers:
                try:
                    scores_false = self.bleurt.compute(
                        predictions=[generated] * len(false_answers),
                        references=false_answers
                    )['scores']
                    max_score_false = max(scores_false) if scores_false else 0.0
                except Exception as exc:
                    logger.warning("BLEURT computation failed for false refs: %s", exc)
                    max_score_false = 0.0

            # Calculate diff score: max(scores_true) - max(scores_false)
            diff_score = max_score_true - max_score_false

            scored.append(self._with_score(item, max_score_true, max_score_false, diff_score))

        return scored

    @staticmethod
    def _with_score(item: Dict, max_true: float, max_false: float, diff: float) -> Dict:
        enriched = dict(item)
        enriched["bleurt_max"] = max_true
        enriched["bleurt_max_false"] = max_false
        enriched["bleurt_diff"] = diff
        enriched["bleurt_acc"] = 1 if diff > 0 else 0
        return enriched
