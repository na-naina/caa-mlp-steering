model:
  name: google/gemma-3-12b-pt
  family: gemma3
  layer: 18  # Changed from 24 (zero-norm issue)
  dtype: bfloat16
  device_map: auto  # Natural splitting across 2 GPUs

slurm:
  gpus: 2  # Natural model distribution
  cpus: 12
  mem_gb: 128
  time: "05:00:00"

steering:
  enabled_variants: [baseline, steered, mlp_gen]  # Skip mlp_mc (ineffective)

# Reduce batch sizes for MLP training
training:
  gen_mlp:
    batch_size: 2  # Reduced from 4
    epochs: 2
    steps_per_epoch: 40
  mc_mlp:
    batch_size: 4  # Reduced from 8
    epochs: 2
    steps_per_epoch: 50

# TruthfulQA alignment evaluation
evaluation:
  preset: qa  # Use QA preset with few-shot examples

  # Truthfulness judge
  judge:
    mode: zero_shot
    model: google/gemma-3-12b-it
    max_new_tokens: 128
    dtype: bfloat16
    device_map: cuda:1  # Use second GPU

  # Informativeness judge
  informativeness:
    enabled: true
    mode: zero_shot
    model: google/gemma-3-12b-it
    max_new_tokens: 128
    dtype: bfloat16
    device_map: cuda:1  # Share with truth judge

  # Semantic similarity with false refs
  semantic:
    enabled: true
    use_false_refs: true
